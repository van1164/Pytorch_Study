{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/van1164/Pytorch_Study/blob/main/lab06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNBuWLpyR_d6",
        "outputId": "847e239b-4a76-4d10-ae51-acd8a280ea81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 62209988.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./',\n",
        "                                             train=True,\n",
        "                                             transform = transform,\n",
        "                                             download =True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './',\n",
        "                                            train = False,\n",
        "                                            transform =transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = 100,\n",
        "                                           shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                          batch_size =100,\n",
        "                                          shuffle =False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKBhbGxMSYUv"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride =1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.residual_fuction = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "\n",
        "    )\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "\n",
        "\n",
        "    if stride !=1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels,kernel_size=1,stride=stride,bias=False),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxPool  = nn.MaxPool2d(4)\n",
        "    self.fc = nn.Linear(out_channels*8*8,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.residual_fuction(x)+self.shortcut(x)\n",
        "    x= self.relu(x)\n",
        "    return x\n",
        "model = BasicBlock(3,64)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqMIVudMc5Xi"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        ouputs = model(images)\n",
        "        loss = criterion(ouputs,labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1)%100 ==0:\n",
        "            print(\"Epoch [{}/{}], step [{}/{}] Loss: {:4f}\".format(epoch+1,num_epochs,i+1,total_step,loss.item()))\n",
        "    \n",
        "    if (epoch+1)%20 == 0:\n",
        "        curr_lr /=3\n",
        "        update_lr(optimizer,curr_lr)\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total =0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data,1)\n",
        "            total +=labels.size(0)\n",
        "            correct +=(predicted == labels).sum().item()\n",
        "\n",
        "        print('Accuracy of the model on the test images: {}%'.format(100*correct/total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNm6C8Thlvbo"
      },
      "source": [
        "Batch\n",
        "Normalization\n",
        "외에\n",
        "아래의\n",
        "4\n",
        "개\n",
        "이상의\n",
        "Normalization\n",
        "방법에\n",
        "대\n",
        "해\n",
        "찾아보고\n",
        ",\n",
        "구현한\n",
        "결과를\n",
        "보이시오"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEFYw_dMlzi8"
      },
      "source": [
        "1. Batch Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jboSv_F9dGbf"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_block, num_classes =10, init_weights =True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels=64\n",
        "\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "    )\n",
        "\n",
        "    self.conv2_x = self._make_layer(block,64,num_block[0],1)\n",
        "    self.conv3_x = self._make_layer(block,128,num_block[1],2)\n",
        "    self.conv4_x = self._make_layer(block,256,num_block[2],2)\n",
        "    self.conv5_x = self._make_layer(block,512,num_block[3],2)\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512,num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self,block,out_channels,num_blocks,stride):\n",
        "    strides = [stride]+[1]*(num_blocks-1)\n",
        "    layer=[]\n",
        "    for stride in strides :\n",
        "      layer.append(block(self.in_channels,out_channels,stride))\n",
        "      self.in_channels = out_channels\n",
        "\n",
        "    return nn.Sequential(*layer)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.conv1(x)\n",
        "    output = self.conv2_x(output)\n",
        "    x = self.conv3_x(output)\n",
        "    x = self.conv4_x(x)\n",
        "    x= self.conv5_x(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x=x.view(x.size(0),-1)\n",
        "    x= self.fc(x)\n",
        "    return x\n",
        "model = ResNet(BasicBlock,[2,2,2,2])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q3whGUYg6w8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "a63860a2-e1bd-418b-c12d-1c12e94a5f27"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ead2d514d3f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1379cee5c156>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(0,num_epochs):\n",
        "  train(epoch)\n",
        "  test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TefTO-vl5VM"
      },
      "source": [
        "2. Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQvH-bQWl2sd"
      },
      "outputs": [],
      "source": [
        "class ResNetL(nn.Module):\n",
        "  def __init__(self, block, num_block, num_classes =10, init_weights =True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels=64\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False),\n",
        "      nn.LayerNorm((self.in_channels,16,16)),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "    )\n",
        "\n",
        "    self.conv2_x = self._make_layer(block,64,num_block[0],1)\n",
        "    self.conv3_x = self._make_layer(block,128,num_block[1],2)\n",
        "    self.conv4_x = self._make_layer(block,256,num_block[2],2)\n",
        "    self.conv5_x = self._make_layer(block,512,num_block[3],2)\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512,num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self,block,out_channels,num_blocks,stride):\n",
        "    strides = [stride]+[1]*(num_blocks-1)\n",
        "    layer=[]\n",
        "    for stride in strides :\n",
        "      layer.append(block(self.in_channels,out_channels,stride))\n",
        "      self.in_channels = out_channels\n",
        "\n",
        "    return nn.Sequential(*layer)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.conv1(x)\n",
        "    output = self.conv2_x(output)\n",
        "    x = self.conv3_x(output)\n",
        "    x = self.conv4_x(x)\n",
        "    x= self.conv5_x(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x=x.view(x.size(0),-1)\n",
        "    x= self.fc(x)\n",
        "    return x\n",
        "model = ResNetL(BasicBlock,[2,2,2,2])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykscws9znAlV"
      },
      "outputs": [],
      "source": [
        "for epoch in range(0,num_epochs):\n",
        "  train(epoch)\n",
        "  test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "InstanceNorm"
      ],
      "metadata": {
        "id": "wY0Q1-1xFfGC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-KRr5QdnBEb"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_block, num_classes =10, init_weights =True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels=64\n",
        "\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False),\n",
        "      nn.InstanceNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "    )\n",
        "\n",
        "    self.conv2_x = self._make_layer(block,64,num_block[0],1)\n",
        "    self.conv3_x = self._make_layer(block,128,num_block[1],2)\n",
        "    self.conv4_x = self._make_layer(block,256,num_block[2],2)\n",
        "    self.conv5_x = self._make_layer(block,512,num_block[3],2)\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512,num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self,block,out_channels,num_blocks,stride):\n",
        "    strides = [stride]+[1]*(num_blocks-1)\n",
        "    layer=[]\n",
        "    for stride in strides :\n",
        "      layer.append(block(self.in_channels,out_channels,stride))\n",
        "      self.in_channels = out_channels\n",
        "\n",
        "    return nn.Sequential(*layer)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.conv1(x)\n",
        "    output = self.conv2_x(output)\n",
        "    x = self.conv3_x(output)\n",
        "    x = self.conv4_x(x)\n",
        "    x= self.conv5_x(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x=x.view(x.size(0),-1)\n",
        "    x= self.fc(x)\n",
        "    return x\n",
        "model = ResNet(BasicBlock,[2,2,2,2])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht7WoMwHs7E5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "eaf7f49e-9174-407b-f9bb-d6eb84ef00e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], step [100/500] Loss: 1.834224\n",
            "Epoch [1/10], step [200/500] Loss: 1.522249\n",
            "Epoch [1/10], step [300/500] Loss: 1.548688\n",
            "Epoch [1/10], step [400/500] Loss: 1.430933\n",
            "Epoch [1/10], step [500/500] Loss: 1.299627\n",
            "Accuracy of the model on the test images: 51.44%\n",
            "Epoch [2/10], step [100/500] Loss: 1.859818\n",
            "Epoch [2/10], step [200/500] Loss: 1.523593\n",
            "Epoch [2/10], step [300/500] Loss: 1.477445\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ead2d514d3f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1379cee5c156>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(0,num_epochs):\n",
        "  train(epoch)\n",
        "  test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GroupNorm\n"
      ],
      "metadata": {
        "id": "GyO3f5stFcE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_block, num_classes =10, init_weights =True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels=64\n",
        "\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False),\n",
        "      nn.GroupNorm(64,64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "    )\n",
        "\n",
        "    self.conv2_x = self._make_layer(block,64,num_block[0],1)\n",
        "    self.conv3_x = self._make_layer(block,128,num_block[1],2)\n",
        "    self.conv4_x = self._make_layer(block,256,num_block[2],2)\n",
        "    self.conv5_x = self._make_layer(block,512,num_block[3],2)\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512,num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self,block,out_channels,num_blocks,stride):\n",
        "    strides = [stride]+[1]*(num_blocks-1)\n",
        "    layer=[]\n",
        "    for stride in strides :\n",
        "      layer.append(block(self.in_channels,out_channels,stride))\n",
        "      self.in_channels = out_channels\n",
        "\n",
        "    return nn.Sequential(*layer)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.conv1(x)\n",
        "    output = self.conv2_x(output)\n",
        "    x = self.conv3_x(output)\n",
        "    x = self.conv4_x(x)\n",
        "    x= self.conv5_x(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x=x.view(x.size(0),-1)\n",
        "    x= self.fc(x)\n",
        "    return x\n",
        "model = ResNet(BasicBlock,[2,2,2,2])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ],
      "metadata": {
        "id": "8SHOmIpcEU5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0,num_epochs):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "iRsv_HCcFZsc",
        "outputId": "222a8272-8862-4f7b-a733-416555911f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], step [100/500] Loss: 1.653610\n",
            "Epoch [1/10], step [200/500] Loss: 1.376367\n",
            "Epoch [1/10], step [300/500] Loss: 1.764217\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ead2d514d3f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1379cee5c156>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet34를 구현하시오."
      ],
      "metadata": {
        "id": "TMuQT9DbGBsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet34(nn.Module):\n",
        "  def __init__(self, block, num_block, num_classes =10, init_weights =True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels=64\n",
        "    self.conv1 =nn.Sequential(\n",
        "      nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "    )\n",
        "    self.conv2_x = self._make_layer(block,64,num_block[0],3)\n",
        "    self.conv3_x = self._make_layer(block,128,num_block[1],4)\n",
        "    self.conv4_x = self._make_layer(block,256,num_block[2],6)\n",
        "    self.conv5_x = self._make_layer(block,512,num_block[3],3)\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512,num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self,block,out_channels,num_blocks,stride):\n",
        "    strides = [stride]+[1]*(num_blocks-1)\n",
        "    layer=[]\n",
        "    for stride in strides :\n",
        "      layer.append(block(self.in_channels,out_channels,stride))\n",
        "      self.in_channels = out_channels\n",
        "\n",
        "    return nn.Sequential(*layer)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.conv1(x)\n",
        "    output = self.conv2_x(output)\n",
        "    x = self.conv3_x(output)\n",
        "    x = self.conv4_x(x)\n",
        "    x= self.conv5_x(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x=x.view(x.size(0),-1)\n",
        "    x= self.fc(x)\n",
        "    return x\n",
        "model = ResNet34(BasicBlock,[2,2,2,2])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ],
      "metadata": {
        "id": "q-9A9yfgFbO7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0,num_epochs):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq0dlW6lQdN_",
        "outputId": "edbcc7df-7524-4a97-bf2e-36add2046f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], step [100/500] Loss: 1.844864\n",
            "Epoch [1/10], step [200/500] Loss: 1.575526\n",
            "Epoch [1/10], step [300/500] Loss: 1.558331\n",
            "Epoch [1/10], step [400/500] Loss: 1.424734\n",
            "Epoch [1/10], step [500/500] Loss: 1.501102\n",
            "Accuracy of the model on the test images: 49.91%\n",
            "Epoch [2/10], step [100/500] Loss: 1.980172\n",
            "Epoch [2/10], step [200/500] Loss: 1.546011\n",
            "Epoch [2/10], step [300/500] Loss: 1.404390\n",
            "Epoch [2/10], step [400/500] Loss: 1.484022\n",
            "Epoch [2/10], step [500/500] Loss: 1.253393\n",
            "Accuracy of the model on the test images: 51.97%\n",
            "Epoch [3/10], step [100/500] Loss: 1.154403\n",
            "Epoch [3/10], step [200/500] Loss: 1.659832\n",
            "Epoch [3/10], step [300/500] Loss: 1.284458\n",
            "Epoch [3/10], step [400/500] Loss: 1.341085\n",
            "Epoch [3/10], step [500/500] Loss: 1.363340\n",
            "Accuracy of the model on the test images: 55.71%\n",
            "Epoch [4/10], step [100/500] Loss: 1.232064\n",
            "Epoch [4/10], step [200/500] Loss: 1.359948\n",
            "Epoch [4/10], step [300/500] Loss: 1.064928\n",
            "Epoch [4/10], step [400/500] Loss: 1.394545\n",
            "Epoch [4/10], step [500/500] Loss: 1.329339\n",
            "Accuracy of the model on the test images: 57.54%\n",
            "Epoch [5/10], step [100/500] Loss: 1.203462\n",
            "Epoch [5/10], step [200/500] Loss: 1.234548\n",
            "Epoch [5/10], step [300/500] Loss: 1.179896\n",
            "Epoch [5/10], step [400/500] Loss: 1.096885\n",
            "Epoch [5/10], step [500/500] Loss: 1.127815\n",
            "Accuracy of the model on the test images: 60.74%\n",
            "Epoch [6/10], step [100/500] Loss: 1.159091\n",
            "Epoch [6/10], step [200/500] Loss: 1.082028\n",
            "Epoch [6/10], step [300/500] Loss: 1.268781\n",
            "Epoch [6/10], step [400/500] Loss: 1.129362\n",
            "Epoch [6/10], step [500/500] Loss: 1.222349\n",
            "Accuracy of the model on the test images: 63.06%\n",
            "Epoch [7/10], step [100/500] Loss: 1.207668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "50\n",
        "층\n",
        "이상의\n",
        "ResNet\n",
        "은\n",
        "매개변수\n",
        "사용을\n",
        "최소화\n",
        "하기\n",
        "위해\n",
        "BottleNeck\n",
        "을\n",
        "이용한다\n",
        ".\n",
        "BottleNeck\n",
        "을\n",
        "구현하고\n",
        ",\n",
        "이를\n",
        "이용하여\n",
        "ResNet50\n",
        "도\n",
        "구현하시\n",
        "오\n",
        "(\n",
        "구현\n",
        "결과를\n",
        "CIFAR10\n",
        "에\n",
        "학습\n",
        "/\n",
        "검증하시오\n",
        ")"
      ],
      "metadata": {
        "id": "r-CPSSEQSZUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride =1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.residual_fuction = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_channels,out_channels*4,kernel_size=1,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels*4),\n",
        "        nn.ReLU(),\n",
        "        \n",
        "\n",
        "    )\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "\n",
        "\n",
        "    if stride !=1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels*4,kernel_size=1,stride=stride,bias=False),\n",
        "        nn.BatchNorm2d(out_channels*4)\n",
        "      )\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxPool  = nn.MaxPool2d(4)\n",
        "    self.fc = nn.Linear(out_channels*8*8,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.residual_fuction(x)+self.shortcut(x)\n",
        "    x= self.relu(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "fbVwZ4PsXSt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet50(nn.Module):\n",
        "  def __init__(self, block, num_block, num_classes =10, init_weights =True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels=64\n",
        "    self.conv1 =nn.Sequential(\n",
        "      nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=False),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "    )\n",
        "    self.conv2_x = self._make_layer(block,64,num_block[0],3)\n",
        "    self.conv3_x = self._make_layer(block,128,num_block[1],4)\n",
        "    self.conv4_x = self._make_layer(block,256,num_block[2],6)\n",
        "    self.conv5_x = self._make_layer(block,512,num_block[3],3)\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512,num_classes)\n",
        "\n",
        "\n",
        "  def _make_layer(self,block,out_channels,num_blocks,stride):\n",
        "    strides = [stride]+[1]*(num_blocks-1)\n",
        "    layer=[]\n",
        "    for stride in strides :\n",
        "      layer.append(block(self.in_channels,out_channels,stride))\n",
        "      self.in_channels = out_channels\n",
        "\n",
        "    return nn.Sequential(*layer)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = self.conv1(x)\n",
        "    output = self.conv2_x(output)\n",
        "    x = self.conv3_x(output)\n",
        "    x = self.conv4_x(x)\n",
        "    x= self.conv5_x(x)\n",
        "    x = self.avg_pool(x)\n",
        "    x=x.view(x.size(0),-1)\n",
        "    x= self.fc(x)\n",
        "    return x\n",
        "model = ResNet34(BasicBlock,[2,2,2,2])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ],
      "metadata": {
        "id": "9vkAhzSOQeyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0,num_epochs):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "id": "fG59Tlj2ca3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3BrdpftKcc9-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXnq5/LdfHzYzaK/YLjFDr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}