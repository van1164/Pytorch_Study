{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/van1164/Pytorch_Study/blob/main/VGGNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_VD7rhwO-9_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf06853-1bef-448d-bd5c-321cfaa0bfd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./',\n",
        "                                             train=True,\n",
        "                                             transform = transform,\n",
        "                                             download =True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './',\n",
        "                                            train = False,\n",
        "                                            transform =transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = 100,\n",
        "                                           shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                          batch_size =100,\n",
        "                                          shuffle =False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(4)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size =3, stride =1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3,stride=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(1568,10)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x= self.relu(self.conv2(x))\n",
        "        x=self.maxpool(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x=self.fc(x)\n",
        "\n",
        "        return x\n",
        "        "
      ],
      "metadata": {
        "id": "pwVhfvyzXqib"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =VGG().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ],
      "metadata": {
        "id": "_zoM18enXtt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        ouputs = model(images)\n",
        "        loss = criterion(ouputs,labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1)%100 ==0:\n",
        "            print(\"Epoch [{}/{}], step [{}/{}] Loss: {:4f}\".format(epoch+1,num_epochs,i+1,total_step,loss.item()))\n",
        "    \n",
        "    if (epoch+1)%20 == 0:\n",
        "        curr_lr /=3\n",
        "        update_lr(optimizer,curr_lr)\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total =0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data,1)\n",
        "            total +=labels.size(0)\n",
        "            correct +=(predicted == labels).sum().item()\n",
        "\n",
        "        print('Accuracy of the model on the test images: {}%'.format(100*correct/total))\n"
      ],
      "metadata": {
        "id": "i5ViFvBzX7r1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1,num_epochs):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3nKGCBiZ0is",
        "outputId": "da46c297-9b20-4eb3-e948-b53d6e7e24e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], step [100/500] Loss: 1.871859\n",
            "Epoch [2/10], step [200/500] Loss: 1.688863\n",
            "Epoch [2/10], step [300/500] Loss: 1.608249\n",
            "Epoch [2/10], step [400/500] Loss: 1.524811\n",
            "Epoch [2/10], step [500/500] Loss: 1.521932\n",
            "Accuracy of the model on the test images: 49.4\n",
            "Epoch [3/10], step [100/500] Loss: 1.505223\n",
            "Epoch [3/10], step [200/500] Loss: 1.393238\n",
            "Epoch [3/10], step [300/500] Loss: 1.655915\n",
            "Epoch [3/10], step [400/500] Loss: 1.332471\n",
            "Epoch [3/10], step [500/500] Loss: 1.313959\n",
            "Accuracy of the model on the test images: 55.14\n",
            "Epoch [4/10], step [100/500] Loss: 1.423641\n",
            "Epoch [4/10], step [200/500] Loss: 1.242416\n",
            "Epoch [4/10], step [300/500] Loss: 1.394009\n",
            "Epoch [4/10], step [400/500] Loss: 1.508890\n",
            "Epoch [4/10], step [500/500] Loss: 1.372045\n",
            "Accuracy of the model on the test images: 57.86\n",
            "Epoch [5/10], step [100/500] Loss: 1.324079\n",
            "Epoch [5/10], step [200/500] Loss: 1.256518\n",
            "Epoch [5/10], step [300/500] Loss: 1.136456\n",
            "Epoch [5/10], step [400/500] Loss: 1.349916\n",
            "Epoch [5/10], step [500/500] Loss: 1.248829\n",
            "Accuracy of the model on the test images: 60.01\n",
            "Epoch [6/10], step [100/500] Loss: 1.287507\n",
            "Epoch [6/10], step [200/500] Loss: 1.161492\n",
            "Epoch [6/10], step [300/500] Loss: 1.229046\n",
            "Epoch [6/10], step [400/500] Loss: 1.230840\n",
            "Epoch [6/10], step [500/500] Loss: 1.047487\n",
            "Accuracy of the model on the test images: 61.99\n",
            "Epoch [7/10], step [100/500] Loss: 1.355293\n",
            "Epoch [7/10], step [200/500] Loss: 1.177592\n",
            "Epoch [7/10], step [300/500] Loss: 1.273692\n",
            "Epoch [7/10], step [400/500] Loss: 1.161192\n",
            "Epoch [7/10], step [500/500] Loss: 1.092956\n",
            "Accuracy of the model on the test images: 62.59\n",
            "Epoch [8/10], step [100/500] Loss: 1.035822\n",
            "Epoch [8/10], step [200/500] Loss: 0.987937\n",
            "Epoch [8/10], step [300/500] Loss: 1.115266\n",
            "Epoch [8/10], step [400/500] Loss: 1.238045\n",
            "Epoch [8/10], step [500/500] Loss: 1.256589\n",
            "Accuracy of the model on the test images: 64.15\n",
            "Epoch [9/10], step [100/500] Loss: 1.126198\n",
            "Epoch [9/10], step [200/500] Loss: 0.974917\n",
            "Epoch [9/10], step [300/500] Loss: 1.043150\n",
            "Epoch [9/10], step [400/500] Loss: 1.258395\n",
            "Epoch [9/10], step [500/500] Loss: 0.933770\n",
            "Accuracy of the model on the test images: 65.74\n",
            "Epoch [10/10], step [100/500] Loss: 1.264774\n",
            "Epoch [10/10], step [200/500] Loss: 1.111533\n",
            "Epoch [10/10], step [300/500] Loss: 1.090139\n",
            "Epoch [10/10], step [400/500] Loss: 1.134098\n",
            "Epoch [10/10], step [500/500] Loss: 1.141293\n",
            "Accuracy of the model on the test images: 65.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "85y3lR7dfitV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rG7cm-bwhLeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n",
        "데이터\n",
        "전처리로\n",
        "정규화\n",
        "(normalization)\n",
        "를\n",
        "수행하시오\n"
      ],
      "metadata": {
        "id": "EKbON0X1kSqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def calculate_norm(dataset):\n",
        "    mean_ = np.array([np.mean(x.numpy(), axis=(1, 2)) for x, _ in dataset])\n",
        "    mean_r = mean_[:, 0].mean()\n",
        "    mean_g = mean_[:, 1].mean()\n",
        "    mean_b = mean_[:, 2].mean()\n",
        "\n",
        "    std_ = np.array([np.std(x.numpy(), axis=(1, 2)) for x, _ in dataset])\n",
        "    std_r = std_[:, 0].mean()\n",
        "    std_g = std_[:, 1].mean()\n",
        "    std_b = std_[:, 2].mean()\n",
        "    \n",
        "    return (mean_r, mean_g, mean_b), (std_r, std_g, std_b)\n",
        "\n",
        "mean, pyo = calculate_norm(train_dataset)"
      ],
      "metadata": {
        "id": "prnl1VXJiUZM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,pyo)\n",
        "])\n"
      ],
      "metadata": {
        "id": "kD4xwXdRjyPX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(root='./',\n",
        "                                             train=True,\n",
        "                                             transform = transform,\n",
        "                                             download =True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './',\n",
        "                                            train = False,\n",
        "                                            transform =transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = 100,\n",
        "                                           shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                          batch_size =100,\n",
        "                                          shuffle =False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2yDTatRkRsZ",
        "outputId": "a19a3048-965a-4451-ca89-80c3f25e4703"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =VGG().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)"
      ],
      "metadata": {
        "id": "ZJIlYrT6nZwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1,num_epochs):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld7ArltdkbKR",
        "outputId": "7d85481d-0e3f-4c41-e584-ad4c55e079b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], step [100/500] Loss: 1.814100\n",
            "Epoch [2/10], step [200/500] Loss: 1.595370\n",
            "Epoch [2/10], step [300/500] Loss: 1.643204\n",
            "Epoch [2/10], step [400/500] Loss: 1.461335\n",
            "Epoch [2/10], step [500/500] Loss: 1.616809\n",
            "Accuracy of the model on the test images: 15.89%\n",
            "Epoch [3/10], step [100/500] Loss: 1.269336\n",
            "Epoch [3/10], step [200/500] Loss: 1.346250\n",
            "Epoch [3/10], step [300/500] Loss: 1.289133\n",
            "Epoch [3/10], step [400/500] Loss: 1.103372\n",
            "Epoch [3/10], step [500/500] Loss: 1.183114\n",
            "Accuracy of the model on the test images: 22.25%\n",
            "Epoch [4/10], step [100/500] Loss: 1.251787\n",
            "Epoch [4/10], step [200/500] Loss: 1.298890\n",
            "Epoch [4/10], step [300/500] Loss: 1.202730\n",
            "Epoch [4/10], step [400/500] Loss: 1.262516\n",
            "Epoch [4/10], step [500/500] Loss: 1.190983\n",
            "Accuracy of the model on the test images: 19.35%\n",
            "Epoch [5/10], step [100/500] Loss: 0.984529\n",
            "Epoch [5/10], step [200/500] Loss: 1.122375\n",
            "Epoch [5/10], step [300/500] Loss: 1.203191\n",
            "Epoch [5/10], step [400/500] Loss: 1.113235\n",
            "Epoch [5/10], step [500/500] Loss: 1.176035\n",
            "Accuracy of the model on the test images: 22.51%\n",
            "Epoch [6/10], step [100/500] Loss: 0.905676\n",
            "Epoch [6/10], step [200/500] Loss: 1.251104\n",
            "Epoch [6/10], step [300/500] Loss: 1.115400\n",
            "Epoch [6/10], step [400/500] Loss: 1.072627\n",
            "Epoch [6/10], step [500/500] Loss: 1.133678\n",
            "Accuracy of the model on the test images: 19.01%\n",
            "Epoch [7/10], step [100/500] Loss: 1.368168\n",
            "Epoch [7/10], step [200/500] Loss: 0.941165\n",
            "Epoch [7/10], step [300/500] Loss: 1.083938\n",
            "Epoch [7/10], step [400/500] Loss: 1.095234\n",
            "Epoch [7/10], step [500/500] Loss: 0.926057\n",
            "Accuracy of the model on the test images: 25.72%\n",
            "Epoch [8/10], step [100/500] Loss: 0.941599\n",
            "Epoch [8/10], step [200/500] Loss: 1.051930\n",
            "Epoch [8/10], step [300/500] Loss: 1.138717\n",
            "Epoch [8/10], step [400/500] Loss: 1.084938\n",
            "Epoch [8/10], step [500/500] Loss: 1.082864\n",
            "Accuracy of the model on the test images: 20.15%\n",
            "Epoch [9/10], step [100/500] Loss: 0.900173\n",
            "Epoch [9/10], step [200/500] Loss: 0.983508\n",
            "Epoch [9/10], step [300/500] Loss: 0.891140\n",
            "Epoch [9/10], step [400/500] Loss: 0.920683\n",
            "Epoch [9/10], step [500/500] Loss: 1.147507\n",
            "Accuracy of the model on the test images: 23.7%\n",
            "Epoch [10/10], step [100/500] Loss: 1.020122\n",
            "Epoch [10/10], step [200/500] Loss: 0.933302\n",
            "Epoch [10/10], step [300/500] Loss: 1.013094\n",
            "Epoch [10/10], step [400/500] Loss: 1.001312\n",
            "Epoch [10/10], step [500/500] Loss: 1.078516\n",
            "Accuracy of the model on the test images: 16.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3\n",
        "아래\n",
        "VGG\n",
        "모델을\n",
        "구현하고\n",
        "CIFAR\n",
        "-\n",
        "100\n",
        "데이터를\n",
        "학습하시오\n",
        ".\n",
        "학습\n",
        "결과\n",
        "를\n",
        "report\n",
        "하시오"
      ],
      "metadata": {
        "id": "94_pqT2mklco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG_3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG_3, self).__init__()\n",
        "\n",
        "        self.block2_1= nn.Conv2d(in_channels=3, out_channels=64, kernel_size =3, stride =1)\n",
        "        self.block2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size =3, stride =1)\n",
        "        self.block3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size =3, stride =1)\n",
        "        self.block3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size =3, stride =1)\n",
        "        self.block3_3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size =3, stride =1)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(2048,1024)\n",
        "        self.fc2 = nn.Linear(1024,10)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.block2_1(x))\n",
        "        x = self.relu(self.block2_2(x))\n",
        "        x=self.maxpool(x)\n",
        "        x = self.relu(self.block3_1(x))\n",
        "        x = self.relu(self.block3_2(x))\n",
        "        x = self.relu(self.block3_3(x))\n",
        "        x=self.maxpool(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x=self.fc1(x)\n",
        "        x=self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "gNPA02MJkjjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =VGG_3().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate"
      ],
      "metadata": {
        "id": "XNWDcp9qu_5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1,num_epochs):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW4xo_aovFAB",
        "outputId": "ca6144fe-57d7-471f-d7a2-53b3c0a49645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], step [100/500] Loss: 1.777136\n",
            "Epoch [2/10], step [200/500] Loss: 1.383581\n",
            "Epoch [2/10], step [300/500] Loss: 1.306512\n",
            "Epoch [2/10], step [400/500] Loss: 1.478821\n",
            "Epoch [2/10], step [500/500] Loss: 1.261651\n",
            "Accuracy of the model on the test images: 14.68%\n",
            "Epoch [3/10], step [100/500] Loss: 1.126640\n",
            "Epoch [3/10], step [200/500] Loss: 1.271836\n",
            "Epoch [3/10], step [300/500] Loss: 1.111669\n",
            "Epoch [3/10], step [400/500] Loss: 0.956812\n",
            "Epoch [3/10], step [500/500] Loss: 1.225571\n",
            "Accuracy of the model on the test images: 17.1%\n",
            "Epoch [4/10], step [100/500] Loss: 1.076818\n",
            "Epoch [4/10], step [200/500] Loss: 1.095790\n",
            "Epoch [4/10], step [300/500] Loss: 0.974246\n",
            "Epoch [4/10], step [400/500] Loss: 0.634796\n",
            "Epoch [4/10], step [500/500] Loss: 0.879453\n",
            "Accuracy of the model on the test images: 21.73%\n",
            "Epoch [5/10], step [100/500] Loss: 0.904998\n",
            "Epoch [5/10], step [200/500] Loss: 0.740342\n",
            "Epoch [5/10], step [300/500] Loss: 0.821802\n",
            "Epoch [5/10], step [400/500] Loss: 0.929241\n",
            "Epoch [5/10], step [500/500] Loss: 0.845523\n",
            "Accuracy of the model on the test images: 24.99%\n",
            "Epoch [6/10], step [100/500] Loss: 0.971084\n",
            "Epoch [6/10], step [200/500] Loss: 0.818877\n",
            "Epoch [6/10], step [300/500] Loss: 0.927569\n",
            "Epoch [6/10], step [400/500] Loss: 0.701119\n",
            "Epoch [6/10], step [500/500] Loss: 0.758454\n",
            "Accuracy of the model on the test images: 25.74%\n",
            "Epoch [7/10], step [100/500] Loss: 0.712010\n",
            "Epoch [7/10], step [200/500] Loss: 0.866499\n",
            "Epoch [7/10], step [300/500] Loss: 0.741028\n",
            "Epoch [7/10], step [400/500] Loss: 0.774244\n",
            "Epoch [7/10], step [500/500] Loss: 0.733964\n",
            "Accuracy of the model on the test images: 24.99%\n",
            "Epoch [8/10], step [100/500] Loss: 0.537400\n",
            "Epoch [8/10], step [200/500] Loss: 0.916152\n",
            "Epoch [8/10], step [300/500] Loss: 0.735672\n",
            "Epoch [8/10], step [400/500] Loss: 0.657426\n",
            "Epoch [8/10], step [500/500] Loss: 0.539306\n",
            "Accuracy of the model on the test images: 25.02%\n",
            "Epoch [9/10], step [100/500] Loss: 0.722063\n",
            "Epoch [9/10], step [200/500] Loss: 0.679249\n",
            "Epoch [9/10], step [300/500] Loss: 0.670770\n",
            "Epoch [9/10], step [400/500] Loss: 0.741576\n",
            "Epoch [9/10], step [500/500] Loss: 0.699597\n",
            "Accuracy of the model on the test images: 26.6%\n",
            "Epoch [10/10], step [100/500] Loss: 0.645137\n",
            "Epoch [10/10], step [200/500] Loss: 0.684476\n",
            "Epoch [10/10], step [300/500] Loss: 0.732959\n",
            "Epoch [10/10], step [400/500] Loss: 0.572574\n",
            "Epoch [10/10], step [500/500] Loss: 0.495209\n",
            "Accuracy of the model on the test images: 28.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\n",
        "이전\n",
        "문제에서\n",
        "구현한\n",
        "VGG\n",
        "모델을\n",
        "튜닝하시오"
      ],
      "metadata": {
        "id": "r4nmMM0Evm3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "maxpool stride 2"
      ],
      "metadata": {
        "id": "o-TxuhxRz9-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG_3_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG_3_2, self).__init__()\n",
        "        self.block2_1= nn.Conv2d(in_channels=3, out_channels=64, kernel_size =3, stride =1)\n",
        "        self.block2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size =3, stride =1)\n",
        "        self.block3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size =3, stride =1)\n",
        "        self.block3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size =3, stride =1)\n",
        "        self.block3_3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size =3, stride =1)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.fc1 = nn.Linear(2048,1024)\n",
        "        self.fc2 = nn.Linear(1024,10)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.block2_1(x))\n",
        "        x = self.relu(self.block2_2(x))\n",
        "        x=self.maxpool(x)\n",
        "        x = self.relu(self.block3_1(x))\n",
        "        x = self.relu(self.block3_2(x))\n",
        "        x = self.relu(self.block3_3(x))\n",
        "        x=self.maxpool(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x=self.fc1(x)\n",
        "        x=self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SzGnEFK8xGyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "\n",
        "model =VGG_3_2().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "\n",
        "for epoch in range(1,num_epochs):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "NCweQLw00WM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "maxpool stride 3"
      ],
      "metadata": {
        "id": "JTemKqit1aUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG_3_3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG_3_3, self).__init__()\n",
        "        self.block2_1= nn.Conv2d(in_channels=3, out_channels=64, kernel_size =3, stride =1)\n",
        "        self.block2_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size =3, stride =1)\n",
        "        self.block3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size =3, stride =1)\n",
        "        self.block3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size =3, stride =1)\n",
        "        self.block3_3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size =3, stride =1)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=3)\n",
        "        self.fc1 = nn.Linear(128,64)\n",
        "        self.fc2 = nn.Linear(64,10)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.block2_1(x))\n",
        "        x = self.relu(self.block2_2(x))\n",
        "        x=self.maxpool(x)\n",
        "        x = self.relu(self.block3_1(x))\n",
        "        x = self.relu(self.block3_2(x))\n",
        "        x = self.relu(self.block3_3(x))\n",
        "        x=self.maxpool(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x=self.fc1(x)\n",
        "        x=self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "a_8shj5j1Zl4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "\n",
        "model =VGG_3_3().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "\n",
        "def update_lr(optimzer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "\n",
        "for epoch in range(1,num_epochs):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2UFzFeP1gTh",
        "outputId": "d08e7f41-1826-4d71-9ff8-b1d632fefe47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], step [100/500] Loss: 2.113459\n",
            "Epoch [2/10], step [200/500] Loss: 2.057937\n",
            "Epoch [2/10], step [300/500] Loss: 1.774823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmOj1Uq92MxP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}